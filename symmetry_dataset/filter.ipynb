{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch \n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_lines(input, line_permutation_order, tokenized_line_inds):\n",
    "    input_len = len(input)\n",
    "    permute_indices = torch.zeros(input_len).to(input.device).detach()\n",
    "    \n",
    "    curr_ind = 0\n",
    "    for new_line_num in line_permutation_order: \n",
    "        line_beg, line_end = tokenized_line_inds[new_line_num], tokenized_line_inds[new_line_num+1]\n",
    "        line_len = line_end - line_beg\n",
    "        permute_indices[curr_ind:curr_ind+line_len] = torch.arange(line_beg, line_end)\n",
    "        curr_ind += line_len\n",
    "        \n",
    "    permuted_input = torch.index_select(input, 0, permute_indices.to(torch.long))\n",
    "    return permuted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokenize(data_path='/home/albertjan/equitune/data/data.pkl', num_permutations=4, max_length=1024, tokenizer=\"deepseek-ai/deepseek-coder-1.3b-base\"):\n",
    "    '''\n",
    "    data_path: Path to a .pkl of a list of (code, permutation_orders, label) where\n",
    "    code is a list of strings for each line of code,\n",
    "    permutations is a list of tuples of invariant permutations, and\n",
    "    label is a string\n",
    "\n",
    "    num_permutations: Filter dataset for samples with at least this many permutations\n",
    "\n",
    "    max_length: Filter dataset for samples whose tokenized length is at most this \n",
    "\n",
    "    tokenizer: Tokenizer to use \n",
    "    '''\n",
    "    with open(data_path, 'rb')  as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    dataset = [sample for sample in dataset if len(sample['permutations']) >= num_permutations]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "    res = []\n",
    "    for sample in tqdm(dataset):\n",
    "        code, line_permutation_orders, label = sample['code'], sample['permutations'], sample['label']\n",
    "        tokenized_loc = [ \n",
    "            tokenizer(line_text, return_tensors=\"pt\", add_special_tokens=(line_num==0))['input_ids'][0]\n",
    "            for line_num, line_text in enumerate(code)\n",
    "        ]\n",
    "        tokenized_loc_len = [len(loc) for loc in tokenized_loc]\n",
    "        tokenized_line_inds = np.array([0] + list(itertools.accumulate(tokenized_loc_len))) # ind of beginning of every line, post-tokenization \n",
    "\n",
    "        input_orig = torch.cat(tokenized_loc) # unpermuted code input\n",
    "        input_len = len(input_orig)\n",
    "        if len(input_orig) > max_length:\n",
    "            continue \n",
    "\n",
    "        num_permutations = len(line_permutation_orders)\n",
    "        # each row contains a permutation of the original code input, we feed this data tensor directly into the model\n",
    "        data = torch.zeros((num_permutations, input_len))\n",
    "        data[0, :] = input_orig\n",
    "        for i in range(1, num_permutations):\n",
    "            # permute the input, fill in next row of data\n",
    "            data[i, :] = permute_lines(\n",
    "                input=input_orig,\n",
    "                line_permutation_order=line_permutation_orders[i],\n",
    "                tokenized_line_inds=tokenized_line_inds\n",
    "            )\n",
    "        label = tokenizer(label, return_tensors=\"pt\", add_special_tokens=False)['input_ids'][0][0].unsqueeze(dim=0)\n",
    "        metadata = {\n",
    "            'line_permutation_orders': line_permutation_orders,\n",
    "            'tokenized_line_inds': tokenized_line_inds,\n",
    "        }\n",
    "        res.append({\n",
    "            'input_ids': data.to(dtype=torch.long),\n",
    "            'label': label, \n",
    "            'metadata': metadata\n",
    "        })\n",
    "\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 26556/26556 [01:54<00:00, 232.43it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered = filter_tokenize(max_length=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_PATH = '/home/albertjan/equitune/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pkl = pickle.dumps(filtered)\n",
    "with open(os.path.join(STORAGE_PATH, 'dataset_4perms.pkl'), 'wb') as f:\n",
    "    f.write(dataset_pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

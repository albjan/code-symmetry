{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch \n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_lines(input, line_permutation_order, tokenized_line_inds):\n",
    "    input_len = len(input)\n",
    "    permute_indices = torch.zeros(input_len).to(input.device).detach()\n",
    "    \n",
    "    curr_ind = 0\n",
    "    for new_line_num in line_permutation_order: \n",
    "        line_beg, line_end = tokenized_line_inds[new_line_num], tokenized_line_inds[new_line_num+1]\n",
    "        line_len = line_end - line_beg\n",
    "        permute_indices[curr_ind:curr_ind+line_len] = torch.arange(line_beg, line_end)\n",
    "        curr_ind += line_len\n",
    "        \n",
    "    permuted_input = torch.index_select(input, 0, permute_indices.to(torch.long))\n",
    "    return permuted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokenize(data_path='/home/albertjan/equitune/data/data.pkl', preprocessing=None, num_permutations=4, max_length=1024, tokenizer=\"deepseek-ai/deepseek-coder-1.3b-base\"):\n",
    "    '''\n",
    "    data_path: Path to a .pkl of a list of (code, permutation_orders, label) where\n",
    "    code is a list of strings for each line of code,\n",
    "    permutations is a list of tuples of invariant permutations, and\n",
    "    label is a string\n",
    "\n",
    "    num_permutations: Filter dataset for samples with at least this many permutations\n",
    "\n",
    "    max_length: Filter dataset for samples whose tokenized length is at most this \n",
    "\n",
    "    tokenizer: Tokenizer to use \n",
    "    '''\n",
    "    with open(data_path, 'rb')  as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    dataset = [sample for sample in dataset if len(sample['permutations']) >= num_permutations]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "    res = []\n",
    "    for sample in tqdm(dataset):\n",
    "        code, line_permutation_orders, label = sample['code'], sample['permutations'], sample['label']\n",
    "        \n",
    "        if preprocessing == \"function_name_prediction\":            \n",
    "            # insert start token and function name \n",
    "            code[0] = \"<｜fim▁begin｜>\" + code[0]\n",
    "            insert_pos = code[0].find(\"(\")\n",
    "            if insert_pos != -1: \n",
    "                code[0] = f\"{code[0][:insert_pos]}<｜fim▁hole｜> {code[0][insert_pos:]}\"\n",
    "            else: \n",
    "                print(\"Didn't find '('\")\n",
    "                continue\n",
    "                # raise ValueError(f\"Line does not contain a '(': {code[0]}\") \n",
    "            # insert end token\n",
    "            code[-1] = code[-1] + \"<｜fim▁end｜>\"\n",
    "\n",
    "        tokenized_loc = [ \n",
    "            tokenizer(line_text, return_tensors=\"pt\", add_special_tokens=(line_num==0))['input_ids'][0]\n",
    "            for line_num, line_text in enumerate(code)\n",
    "        ]\n",
    "        \n",
    "        tokenized_loc_len = [len(loc) for loc in tokenized_loc]\n",
    "        tokenized_line_inds = np.array([0] + list(itertools.accumulate(tokenized_loc_len))) # ind of beginning of every line, post-tokenization \n",
    "\n",
    "        input_orig = torch.cat(tokenized_loc) # unpermuted code input\n",
    "        input_len = len(input_orig)\n",
    "        if len(input_orig) > max_length:\n",
    "            continue \n",
    "\n",
    "        num_permutations = len(line_permutation_orders)\n",
    "        # each row contains a permutation of the original code input, we feed this data tensor directly into the model\n",
    "        data = torch.zeros((num_permutations, input_len))\n",
    "        data[0, :] = input_orig\n",
    "        for i in range(1, num_permutations):\n",
    "            # permute the input, fill in next row of data\n",
    "            data[i, :] = permute_lines(\n",
    "                input=input_orig,\n",
    "                line_permutation_order=line_permutation_orders[i],\n",
    "                tokenized_line_inds=tokenized_line_inds\n",
    "            )\n",
    "        label = tokenizer(label, return_tensors=\"pt\", add_special_tokens=False)['input_ids'][0][0].unsqueeze(dim=0)\n",
    "        metadata = {\n",
    "            'line_permutation_orders': line_permutation_orders,\n",
    "            'tokenized_line_inds': tokenized_line_inds,\n",
    "        }\n",
    "        res.append({\n",
    "            'input_ids': data.to(dtype=torch.long),\n",
    "            'label': label, \n",
    "            'metadata': metadata\n",
    "        })\n",
    "\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 26556/26556 [01:54<00:00, 232.43it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered = filter_tokenize(max_length=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  7%|▋         | 1801/26556 [00:07<01:44, 236.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find '('\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2225/26556 [00:09<01:43, 235.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find '('\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 3047/26556 [00:12<01:39, 235.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find '('\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3192/26556 [00:13<01:51, 209.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find '('\n",
      "Didn't find '('\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4142/26556 [00:17<01:39, 226.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find '('\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 4637/26556 [00:19<01:35, 229.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find '('\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 5230/26556 [00:22<01:32, 230.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find '('\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5377/26556 [00:22<01:31, 231.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find '('\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 7925/26556 [00:32<01:18, 238.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find '('\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 15032/26556 [01:04<00:50, 227.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't find '('\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26556/26556 [01:55<00:00, 230.57it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_function_name_preprocessed = filter_tokenize(max_length=900, preprocessing=\"function_name_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_PATH = '/home/albertjan/equitune/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pkl = pickle.dumps(filtered)\n",
    "with open(os.path.join(STORAGE_PATH, 'dataset_4perms.pkl'), 'wb') as f:\n",
    "    f.write(dataset_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preprocessed_pkl = pickle.dumps(filtered_function_name_preprocessed)\n",
    "with open(os.path.join(STORAGE_PATH, 'dataset_4perms_function_insertion_preprocessed.pkl'), 'wb') as f:\n",
    "    f.write(dataset_preprocessed_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

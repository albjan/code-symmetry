{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aj3051/anaconda3/envs/backpack/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, RandomSampler, random_split\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-5\n",
    "# num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "STORAGE_DIR = '/proj/rcs-hdd/aj3051/symmetry'\n",
    "\n",
    "with open(os.path.join(STORAGE_DIR, 'data_tokenized_4.pkl'), 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "def permute_lines(input, line_permutation_order, tokenized_line_inds):\n",
    "    input_len = len(input)\n",
    "    permute_indices = torch.zeros(input_len).to(input.device).detach()\n",
    "    \n",
    "    curr_ind = 0\n",
    "    for new_line_num in line_permutation_order: \n",
    "        line_beg, line_end = tokenized_line_inds[new_line_num], tokenized_line_inds[new_line_num+1]\n",
    "        line_len = line_end - line_beg\n",
    "        permute_indices[curr_ind:curr_ind+line_len] = torch.arange(line_beg, line_end)\n",
    "        curr_ind += line_len\n",
    "        \n",
    "    permuted_input = torch.index_select(input, 0, permute_indices.to(torch.long))\n",
    "    return permuted_input\n",
    "\n",
    "class CodePermutationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "permutation_dataset = CodePermutationDataset(dataset=dataset)\n",
    "\n",
    "sampler = RandomSampler(dataset)\n",
    "shuffled_indices = list(sampler)\n",
    "shuffled_dataset = torch.utils.data.Subset(dataset, shuffled_indices)\n",
    "train_length = int(0.8 * len(shuffled_dataset))\n",
    "validation_length = int(0.1 * len(shuffled_dataset))\n",
    "test_length = len(shuffled_dataset) - train_length - validation_length\n",
    "\n",
    "train_set, validation_set, test_set = random_split(\n",
    "    shuffled_dataset, [train_length, validation_length, test_length]\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=None, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=None, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=None, shuffle=False)\n",
    "\n",
    "class CodePermutationEquitune(nn.Module): \n",
    "\n",
    "    def __init__(self, base_model=\"deepseek-ai/deepseek-coder-1.3b-base\", num_permutations=4):\n",
    "        super(CodePermutationEquitune, self).__init__() \n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(base_model, cache_dir=\"/proj/rcs-hdd/aj3051/hf_transformers\") #torch_dtype=torch.float16)\n",
    "        self.num_permutations = num_permutations\n",
    "        module_dict = dict(self.base_model.named_modules())\n",
    "\n",
    "        second_to_last_attn_layer = module_dict['model.layers.22']\n",
    "        second_to_last_attn_layer.register_forward_hook(self.permute_output_hook)\n",
    "\n",
    "        last_attn_layer = module_dict['model.layers.23']\n",
    "        last_attn_layer.register_forward_hook(self.average_output_hook)\n",
    "\n",
    "    def permute_output_hook(self, module, input, output):\n",
    "        hidden_state = output[0]\n",
    "        # print(f'thing being permuted shape: {hidden_state.shape}')\n",
    "        num_permutations, num_tokens, embedding_dim = hidden_state.shape\n",
    "        hidden_state = hidden_state.reshape(hidden_state.shape[0], -1)\n",
    "        # print(f'reshaped hidden state: {hidden_state.shape}')\n",
    "        embedding_orig = hidden_state[0, :]\n",
    "        for i in range(1, num_permutations):\n",
    "    # permute the input, fill in next row of data\n",
    "            hidden_state[i, :] = permute_lines(\n",
    "                input=embedding_orig,\n",
    "                line_permutation_order=self.metadata['line_permutation_orders'][i],\n",
    "                tokenized_line_inds=self.metadata['tokenized_line_inds'] * embedding_dim,\n",
    "            )\n",
    "        hidden_state = hidden_state.reshape(num_permutations, num_tokens, embedding_dim)\n",
    "        # print(f'reshaped hidden state again: {hidden_state.shape}')\n",
    "        return (hidden_state,) + output[1:]\n",
    "\n",
    "    def average_output_hook(self, module, input, output):\n",
    "        hidden_state = output[0]\n",
    "        hidden_state = hidden_state.mean(axis=0).unsqueeze(0)\n",
    "        return (hidden_state,) + output[1:]\n",
    "\n",
    "    def forward(self, input, metadata):\n",
    "        x = input\n",
    "        self.metadata = metadata\n",
    "        x = self.base_model(x)\n",
    "        logits = x.logits[:, -1, :]\n",
    "        return logits\n",
    "    \n",
    "equitune_model = CodePermutationEquitune().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = bnb.optim.Adam8bit(equitune_model.parameters(), lr=lr)\n",
    "\n",
    "eval_every = 4000\n",
    "checkpoint_every = 10000\n",
    "\n",
    "MODEL_DIR = 'f{STORAGE_DIR}/results/'\n",
    "\n",
    "equitune_model.train()\n",
    "step = 0\n",
    "mx_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maj3051\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/local/rcs/aj3051/equitune/wandb/run-20240509_072658-bc6msc56</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aj3051/equitune/runs/bc6msc56' target=\"_blank\">Run 1</a></strong> to <a href='https://wandb.ai/aj3051/equitune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aj3051/equitune' target=\"_blank\">https://wandb.ai/aj3051/equitune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aj3051/equitune/runs/bc6msc56' target=\"_blank\">https://wandb.ai/aj3051/equitune/runs/bc6msc56</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"equitune\",\n",
    "    name='Run 1',\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": num_epochs,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "STORAGE_DIR = '/proj/rcs-hdd/aj3051/symmetry'\n",
    "# FILE_NAME = 'data_four_permutations.pickle'\n",
    "# DATA_PATH = os.path.join(STORAGE_DIR, 'data_four_permutations.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(STORAGE_DIR, FILE_NAME), 'rb') as f:\n",
    "#     og_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(STORAGE_DIR, 'data_tokenized_4.pkl'), 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_filter_dataset(dataset, max_length=1024, tokenizer=\"deepseek-ai/deepseek-coder-1.3b-base\", num_permutations=4):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "#     res = []\n",
    "#     for code, line_permutation_orders, label in tqdm(dataset):\n",
    "#         tokenized_loc = [ \n",
    "#             tokenizer(line_text, return_tensors=\"pt\", add_special_tokens=(line_num==0))['input_ids'][0]\n",
    "#             for line_num, line_text in enumerate(code)\n",
    "#         ]\n",
    "#         tokenized_loc_len = [len(loc) for loc in tokenized_loc]\n",
    "#         tokenized_line_inds = np.array([0] + list(itertools.accumulate(tokenized_loc_len))) # ind of beginning of every line, post-tokenization \n",
    "\n",
    "#         input_orig = torch.cat(tokenized_loc) # unpermuted code input\n",
    "#         input_len = len(input_orig)\n",
    "#         if len(input_orig) > max_length:\n",
    "#             continue \n",
    "\n",
    "#         num_permutations = len(line_permutation_orders)\n",
    "#         # each row contains a permutation of the original code input, we feed this data tensor directly into the model\n",
    "#         data = torch.zeros((num_permutations, input_len))\n",
    "#         data[0, :] = input_orig\n",
    "#         for i in range(1, num_permutations):\n",
    "#             # permute the input, fill in next row of data\n",
    "#             data[i, :] = permute_lines(\n",
    "#                 input=input_orig,\n",
    "#                 line_permutation_order=line_permutation_orders[i],\n",
    "#                 tokenized_line_inds=tokenized_line_inds\n",
    "#             )\n",
    "#         label = tokenizer(label, return_tensors=\"pt\", add_special_tokens=False)['input_ids'][0][0].unsqueeze(dim=0)\n",
    "#         metadata = {\n",
    "#             'line_permutation_orders': line_permutation_orders,\n",
    "#             'tokenized_line_inds': tokenized_line_inds,\n",
    "#         }\n",
    "#         res.append((data.to(dtype=torch.long), label, metadata))\n",
    "\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = tokenize_filter_dataset(dataset=og_dataset, max_length=1024, tokenizer=\"deepseek-ai/deepseek-coder-1.3b-base\", num_permutations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_lines(input, line_permutation_order, tokenized_line_inds):\n",
    "    input_len = len(input)\n",
    "    permute_indices = torch.zeros(input_len).to(input.device).detach()\n",
    "    \n",
    "    curr_ind = 0\n",
    "    for new_line_num in line_permutation_order: \n",
    "        line_beg, line_end = tokenized_line_inds[new_line_num], tokenized_line_inds[new_line_num+1]\n",
    "        line_len = line_end - line_beg\n",
    "        permute_indices[curr_ind:curr_ind+line_len] = torch.arange(line_beg, line_end)\n",
    "        curr_ind += line_len\n",
    "        \n",
    "    permuted_input = torch.index_select(input, 0, permute_indices.to(torch.long))\n",
    "    return permuted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodePermutationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_dataset = CodePermutationDataset(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomSampler(dataset)\n",
    "shuffled_indices = list(sampler)\n",
    "shuffled_dataset = torch.utils.data.Subset(dataset, shuffled_indices)\n",
    "train_length = int(0.8 * len(shuffled_dataset))\n",
    "validation_length = int(0.1 * len(shuffled_dataset))\n",
    "test_length = len(shuffled_dataset) - train_length - validation_length\n",
    "\n",
    "train_set, validation_set, test_set = random_split(\n",
    "    shuffled_dataset, [train_length, validation_length, test_length]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=None, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=None, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodePermutationEquitune(nn.Module): \n",
    "\n",
    "    def __init__(self, base_model=\"deepseek-ai/deepseek-coder-1.3b-base\", num_permutations=4):\n",
    "        super(CodePermutationEquitune, self).__init__() \n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(base_model, cache_dir=\"/proj/rcs-hdd/aj3051/hf_transformers\") #torch_dtype=torch.float16)\n",
    "        self.num_permutations = num_permutations\n",
    "        module_dict = dict(self.base_model.named_modules())\n",
    "\n",
    "        second_to_last_attn_layer = module_dict['model.layers.22']\n",
    "        second_to_last_attn_layer.register_forward_hook(self.permute_output_hook)\n",
    "\n",
    "        last_attn_layer = module_dict['model.layers.23']\n",
    "        last_attn_layer.register_forward_hook(self.average_output_hook)\n",
    "\n",
    "    def permute_output_hook(self, module, input, output):\n",
    "        hidden_state = output[0]\n",
    "        # print(f'thing being permuted shape: {hidden_state.shape}')\n",
    "        num_permutations, num_tokens, embedding_dim = hidden_state.shape\n",
    "        hidden_state = hidden_state.reshape(hidden_state.shape[0], -1)\n",
    "        # print(f'reshaped hidden state: {hidden_state.shape}')\n",
    "        embedding_orig = hidden_state[0, :]\n",
    "        for i in range(1, num_permutations):\n",
    "    # permute the input, fill in next row of data\n",
    "            hidden_state[i, :] = permute_lines(\n",
    "                input=embedding_orig,\n",
    "                line_permutation_order=self.metadata['line_permutation_orders'][i],\n",
    "                tokenized_line_inds=self.metadata['tokenized_line_inds'] * embedding_dim,\n",
    "            )\n",
    "        hidden_state = hidden_state.reshape(num_permutations, num_tokens, embedding_dim)\n",
    "        # print(f'reshaped hidden state again: {hidden_state.shape}')\n",
    "        return (hidden_state,) + output[1:]\n",
    "\n",
    "    def average_output_hook(self, module, input, output):\n",
    "        hidden_state = output[0]\n",
    "        hidden_state = hidden_state.mean(axis=0).unsqueeze(0)\n",
    "        return (hidden_state,) + output[1:]\n",
    "\n",
    "    def forward(self, input, metadata):\n",
    "        x = input\n",
    "        self.metadata = metadata\n",
    "        x = self.base_model(x)\n",
    "        logits = x.logits[:, -1, :]\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "equitune_model = CodePermutationEquitune().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = bnb.optim.Adam8bit(equitune_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_every = 4000\n",
    "checkpoint_every = 10000\n",
    "\n",
    "MODEL_DIR = 'f{STORAGE_DIR}/results/'\n",
    "\n",
    "equitune_model.train()\n",
    "step = 0\n",
    "mx_len = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_val():\n",
    "    val_loss = 0\n",
    "    for batch in enumerate(validation_loader):\n",
    "        input_ids, label, metadata = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        label = label.unsqueeze(dim=0).to(device)\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            output = equitune_model(input_ids, metadata)\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "    val_loss = val_loss / len(validation_set)\n",
    "    wandb.log({\"validation loss: \", val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/105560 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     output \u001b[38;5;241m=\u001b[39m equitune_model(input_ids, metadata)\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output, label)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# wandb.log({\"training loss\": loss})\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# wandb.log({\"sequence length\": input_ids.shape[1]})\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 92\u001b[0m, in \u001b[0;36mCodePermutationEquitune.forward\u001b[0;34m(self, input, metadata)\u001b[0m\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m---> 92\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(x)\n\u001b[1;32m     93\u001b[0m logits \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1183\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1180\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1182\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m   1184\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1185\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1186\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1187\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1188\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1189\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1190\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1191\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1192\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1193\u001b[0m )\n\u001b[1;32m   1195\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1196\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1070\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1061\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1062\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         use_cache,\n\u001b[1;32m   1068\u001b[0m     )\n\u001b[1;32m   1069\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1070\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1071\u001b[0m         hidden_states,\n\u001b[1;32m   1072\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1073\u001b[0m         position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1074\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1075\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1076\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1077\u001b[0m     )\n\u001b[1;32m   1079\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1081\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1574\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1573\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1574\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, result)\n\u001b[1;32m   1576\u001b[0m \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1577\u001b[0m     result \u001b[39m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[6], line 75\u001b[0m, in \u001b[0;36mCodePermutationEquitune.permute_output_hook\u001b[0;34m(self, module, input, output)\u001b[0m\n\u001b[1;32m     72\u001b[0m     embedding_orig \u001b[38;5;241m=\u001b[39m hidden_state[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_permutations):\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# permute the input, fill in next row of data\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m         hidden_state[i, :] \u001b[38;5;241m=\u001b[39m permute_lines(\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39membedding_orig,\n\u001b[1;32m     77\u001b[0m             line_permutation_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mline_permutation_orders\u001b[39m\u001b[38;5;124m'\u001b[39m][i],\n\u001b[1;32m     78\u001b[0m             tokenized_line_inds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_line_inds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m embedding_dim,\n\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m hidden_state\u001b[38;5;241m.\u001b[39mreshape(num_permutations, num_tokens, embedding_dim)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# print(f'reshaped hidden state again: {hidden_state.shape}')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mpermute_lines\u001b[0;34m(input, line_permutation_order, tokenized_line_inds)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpermute_lines\u001b[39m(\u001b[38;5;28minput\u001b[39m, line_permutation_order, tokenized_line_inds):\n\u001b[1;32m     11\u001b[0m     input_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     permute_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_len)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     14\u001b[0m     curr_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m new_line_num \u001b[38;5;129;01min\u001b[39;00m line_permutation_order: \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(range(num_epochs * len(train_set)))\n",
    "\n",
    "for epoch in range(num_epochs):  # Train for 5 epochs\n",
    "    total_loss = 0\n",
    "    for epoch_i, batch in enumerate(train_loader):\n",
    "        input_ids, label, metadata = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        label = label.unsqueeze(dim=0).to(device)\n",
    "        if input_ids.shape[1] > 900:\n",
    "            continue \n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            output = equitune_model(input_ids, metadata)\n",
    "            loss = loss_fn(output, label)\n",
    "        wandb.log({\"training loss\": loss})\n",
    "        wandb.log({\"sequence length\": input_ids.shape[1]})\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        step += 1\n",
    "        if step % eval_every == 0:\n",
    "            get_val()\n",
    "        if step % checkpoint_every == 0:\n",
    "            save_dir = f\"{MODEL_DIR}/model_{step}\"\n",
    "            torch.save({\n",
    "                'epoch': epoch, \n",
    "                'model_state_dict': equitune_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss\n",
    "            }, save_dir)\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "save_dir = f\"{MODEL_DIR}/final_model\"\n",
    "torch.save({\n",
    "    'epoch': epoch, \n",
    "    'model_state_dict': equitune_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss\n",
    "}, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('backpack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e552d307c8781d98a1345404e47beeec67d0c4ac0bb536f85b4d3cc6c64bd723"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aj3051/anaconda3/envs/backpack/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/aj3051/anaconda3/envs/backpack/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_dict = dict(model.named_modules())\n",
    "second_to_last_attn_layer = module_dict['model.layers.22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, t1 = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output_dimensions_hook(module, input, output):\n",
    "    print(len(output))\n",
    "    global t0, t1\n",
    "    t0 = output[0]\n",
    "    t1 = output[1]\n",
    "output_hook_handle = second_to_last_attn_layer.register_forward_hook(print_output_dimensions_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(100, size=(4, 189)).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([[[ 6.1546e-01, -5.7934e-01,  5.3252e-01,  ...,  7.5423e-01,\n",
      "           4.0738e-01, -7.0639e-02],\n",
      "         [ 3.3488e-01, -3.1896e+00,  1.6580e+00,  ..., -6.9320e+00,\n",
      "           1.3070e+00,  2.7097e-01],\n",
      "         [-4.1879e+00, -5.0339e+00, -1.2603e+00,  ..., -3.7969e+00,\n",
      "           4.5087e+00, -3.6279e+00],\n",
      "         ...,\n",
      "         [-5.2951e+00, -3.4597e+00, -1.6850e+00,  ..., -4.0988e+00,\n",
      "           1.7761e+00, -3.3192e+00],\n",
      "         [-4.3696e+00, -1.6827e+00, -3.4729e+00,  ..., -2.1099e+00,\n",
      "           3.2392e-01,  8.4261e-01],\n",
      "         [-3.2664e+00, -2.6713e+00, -4.0747e+00,  ..., -5.3469e+00,\n",
      "           1.2561e+00, -1.8406e-01]],\n",
      "\n",
      "        [[-4.1013e-03, -5.4911e-01,  4.8409e-01,  ...,  1.2295e+00,\n",
      "           1.9161e-01,  2.7434e-01],\n",
      "         [ 2.6457e+00, -3.8138e+00, -2.8669e-01,  ...,  6.1548e-01,\n",
      "          -1.5887e+00,  6.3306e-01],\n",
      "         [ 4.7496e-01, -2.9190e+00, -4.4240e+00,  ...,  8.8504e-02,\n",
      "          -1.0563e+00, -1.6418e+00],\n",
      "         ...,\n",
      "         [-3.5923e+00, -1.3598e+00, -2.7289e+00,  ..., -6.2832e+00,\n",
      "           2.6103e+00, -4.4893e-01],\n",
      "         [-1.8616e+00, -1.6693e+00, -6.0298e+00,  ..., -4.1783e+00,\n",
      "           1.9578e+00, -8.1270e-01],\n",
      "         [-1.5014e+00, -9.0805e-01, -4.1650e+00,  ..., -5.4559e+00,\n",
      "          -7.4551e-02,  6.1422e-01]],\n",
      "\n",
      "        [[-1.9557e-01, -5.0461e-01,  4.6413e-01,  ...,  1.5580e+00,\n",
      "           2.3741e-02,  3.2088e-01],\n",
      "         [-9.0948e-01, -2.7045e-01,  2.8913e-01,  ...,  1.6704e-01,\n",
      "          -3.0439e-01, -1.3987e+00],\n",
      "         [-1.2126e+00, -3.9375e+00,  4.4094e-02,  ..., -1.6698e+00,\n",
      "           4.4854e+00, -3.4104e+00],\n",
      "         ...,\n",
      "         [-2.6887e+00, -2.4076e+00, -4.6808e+00,  ..., -6.2246e+00,\n",
      "           2.6202e+00,  2.9991e+00],\n",
      "         [-2.8185e+00, -1.1494e-01, -4.1667e+00,  ..., -3.4819e+00,\n",
      "          -7.2470e-01,  9.7664e-02],\n",
      "         [-1.5543e+00,  1.2573e+00, -1.6053e+00,  ..., -1.0270e+00,\n",
      "          -5.2305e-01, -1.2480e+00]],\n",
      "\n",
      "        [[ 3.9475e-01,  6.4413e-02,  3.0613e-04,  ...,  3.8338e-01,\n",
      "          -5.7031e-01,  7.0013e-01],\n",
      "         [ 1.3871e+00, -1.6142e+00,  4.8104e-01,  ...,  2.6824e+00,\n",
      "          -5.8795e-02, -3.0826e-01],\n",
      "         [-1.9902e+00, -1.4549e+00, -3.3247e+00,  ..., -1.2010e+00,\n",
      "          -1.9037e+00, -1.8045e+00],\n",
      "         ...,\n",
      "         [-2.7210e+00, -2.1954e+00, -3.2667e+00,  ..., -4.6109e+00,\n",
      "          -1.4419e+00,  1.4339e-01],\n",
      "         [-4.6564e+00, -2.9502e+00, -5.2228e+00,  ..., -4.3575e+00,\n",
      "           3.4211e-01, -1.2867e+00],\n",
      "         [-6.4965e+00, -2.0510e-01, -1.5561e+00,  ..., -4.3493e+00,\n",
      "          -5.6709e-02,  2.0289e+00]]], device='cuda:2', grad_fn=<AddBackward0>)\n",
      "DynamicCache()\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 189, 32256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor = output[0]\n",
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.model.embed_tokens(input)\n",
    "pre_permute = embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.arange(800).reshape(4, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LlamaDecoderLayer.forward() missing 1 required positional argument: 'hidden_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](position_ids\u001b[38;5;241m=\u001b[39mpre_permute)\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaDecoderLayer.forward() missing 1 required positional argument: 'hidden_states'"
     ]
    }
   ],
   "source": [
    "x = model.model.layers[0](position_ids=pre_permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pre_permute \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m23\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     pre_permute \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[i](pre_permute)\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:739\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    738\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 739\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    740\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    741\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    742\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m    743\u001b[0m     past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m    744\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    745\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    746\u001b[0m     cache_position\u001b[39m=\u001b[39mcache_position,\n\u001b[1;32m    747\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    748\u001b[0m )\n\u001b[1;32m    749\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    751\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:644\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    641\u001b[0m key_states \u001b[39m=\u001b[39m key_states\u001b[39m.\u001b[39mview(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_key_value_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    642\u001b[0m value_states \u001b[39m=\u001b[39m value_states\u001b[39m.\u001b[39mview(bsz, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_key_value_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m--> 644\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb(value_states, position_ids)\n\u001b[1;32m    645\u001b[0m query_states, key_states \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\u001b[1;32m    647\u001b[0m \u001b[39m# In case static cache is used, it is an instance attribute.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153\u001b[0m, in \u001b[0;36mLlamaLinearScalingRotaryEmbedding.forward\u001b[0;34m(self, x, position_ids)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, position_ids):\n\u001b[1;32m    152\u001b[0m     \u001b[39m# difference to the original RoPE: a scaling factor is aplied to the position ids\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     position_ids \u001b[39m=\u001b[39m position_ids\u001b[39m.\u001b[39mfloat() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaling_factor\n\u001b[1;32m    154\u001b[0m     cos, sin \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mforward(x, position_ids)\n\u001b[1;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m cos, sin\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "pre_permute = embedding\n",
    "for i in range(23):\n",
    "    pre_permute = model.model.layers[i](pre_permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.8188e-02,  2.0898e-01, -5.9326e-02,  ..., -9.1797e-02,\n",
       "          1.4648e-01,  1.4160e-01],\n",
       "        [-3.1006e-02,  1.1133e-01, -7.4219e-02,  ..., -4.7607e-03,\n",
       "          1.8799e-02,  7.7637e-02],\n",
       "        [ 1.1230e-01,  2.4609e-01,  6.1768e-02,  ..., -1.3086e-01,\n",
       "          4.7852e-02,  6.4453e-02],\n",
       "        ...,\n",
       "        [-2.3499e-03, -6.0120e-03, -6.4373e-05,  ..., -8.0566e-03,\n",
       "         -7.3242e-03, -7.4768e-03],\n",
       "        [ 2.8687e-03,  7.5684e-03,  1.3733e-02,  ...,  1.3924e-04,\n",
       "         -1.5106e-03, -1.0681e-04],\n",
       "        [ 8.6670e-03,  1.8997e-03,  5.4169e-04,  ..., -2.4109e-03,\n",
       "         -6.6833e-03, -3.3264e-03]], device='cuda:2', requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-22): 23 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaSdpaAttention(\n",
       "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "      (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "      (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm()\n",
       "    (post_attention_layernorm): LlamaRMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaRMSNorm' object has no attribute 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[0;32m~/anaconda3/envs/backpack/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaRMSNorm' object has no attribute 'weights'"
     ]
    }
   ],
   "source": [
    "model.model.norm.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.modeling_llama.LlamaDecoderLayer"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_dict = dict(model.named_modules())\n",
    "type(module_dict['model.layers.22'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3477, 0.3574, 0.3711,  ..., 0.3691, 0.3750, 0.3672], device='cuda:2',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.norm.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:  MODULE: LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "          (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32256, bias=False)\n",
      ")\n",
      "NAME: model MODULE: LlamaModel(\n",
      "  (embed_tokens): Embedding(32256, 2048)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "        (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.embed_tokens MODULE: Embedding(32256, 2048)\n",
      "NAME: model.layers MODULE: ModuleList(\n",
      "  (0-23): 24 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaSdpaAttention(\n",
      "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "      (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "      (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm()\n",
      "    (post_attention_layernorm): LlamaRMSNorm()\n",
      "  )\n",
      ")\n",
      "NAME: model.layers.0 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.0.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.0.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.0.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.0.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.0.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.0.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.0.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.0.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.0.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.0.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.0.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.0.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.0.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.1 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.1.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.1.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.1.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.1.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.1.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.1.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.1.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.1.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.1.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.1.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.1.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.1.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.1.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.2 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.2.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.2.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.2.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.2.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.2.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.2.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.2.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.2.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.2.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.2.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.2.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.2.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.2.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.3 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.3.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.3.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.3.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.3.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.3.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.3.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.3.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.3.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.3.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.3.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.3.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.3.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.3.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.4 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.4.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.4.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.4.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.4.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.4.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.4.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.4.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.4.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.4.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.4.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.4.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.4.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.4.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.5 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.5.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.5.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.5.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.5.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.5.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.5.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.5.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.5.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.5.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.5.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.5.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.5.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.5.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.6 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.6.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.6.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.6.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.6.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.6.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.6.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.6.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.6.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.6.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.6.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.6.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.6.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.6.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.7 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.7.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.7.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.7.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.7.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.7.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.7.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.7.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.7.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.7.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.7.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.7.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.7.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.7.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.8 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.8.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.8.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.8.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.8.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.8.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.8.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.8.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.8.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.8.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.8.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.8.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.8.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.8.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.9 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.9.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.9.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.9.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.9.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.9.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.9.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.9.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.9.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.9.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.9.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.9.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.9.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.9.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.10 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.10.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.10.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.10.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.10.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.10.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.10.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.10.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.10.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.10.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.10.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.10.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.10.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.10.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.11 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.11.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.11.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.11.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.11.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.11.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.11.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.11.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.11.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.11.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.11.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.11.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.11.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.11.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.12 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.12.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.12.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.12.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.12.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.12.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.12.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.12.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.12.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.12.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.12.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.12.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.12.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.12.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.13 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.13.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.13.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.13.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.13.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.13.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.13.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.13.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.13.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.13.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.13.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.13.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.13.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.13.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.14 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.14.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.14.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.14.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.14.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.14.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.14.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.14.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.14.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.14.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.14.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.14.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.14.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.14.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.15 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.15.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.15.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.15.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.15.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.15.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.15.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.15.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.15.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.15.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.15.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.15.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.15.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.15.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.16 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.16.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.16.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.16.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.16.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.16.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.16.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.16.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.16.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.16.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.16.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.16.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.16.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.16.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.17 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.17.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.17.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.17.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.17.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.17.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.17.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.17.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.17.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.17.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.17.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.17.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.17.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.17.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.18 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.18.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.18.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.18.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.18.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.18.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.18.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.18.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.18.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.18.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.18.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.18.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.18.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.18.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.19 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.19.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.19.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.19.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.19.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.19.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.19.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.19.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.19.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.19.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.19.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.19.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.19.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.19.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.20 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.20.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.20.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.20.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.20.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.20.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.20.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.20.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.20.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.20.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.20.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.20.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.20.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.20.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.21 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.21.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.21.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.21.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.21.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.21.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.21.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.21.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.21.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.21.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.21.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.21.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.21.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.21.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.22 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.22.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.22.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.22.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.22.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.22.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.22.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.22.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.22.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.22.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.22.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.22.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.22.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.22.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.23 MODULE: LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "    (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "NAME: model.layers.23.self_attn MODULE: LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
      ")\n",
      "NAME: model.layers.23.self_attn.q_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.23.self_attn.k_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.23.self_attn.v_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.23.self_attn.o_proj MODULE: Linear(in_features=2048, out_features=2048, bias=False)\n",
      "NAME: model.layers.23.self_attn.rotary_emb MODULE: LlamaLinearScalingRotaryEmbedding()\n",
      "NAME: model.layers.23.mlp MODULE: LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "  (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "NAME: model.layers.23.mlp.gate_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.23.mlp.up_proj MODULE: Linear(in_features=2048, out_features=5504, bias=False)\n",
      "NAME: model.layers.23.mlp.down_proj MODULE: Linear(in_features=5504, out_features=2048, bias=False)\n",
      "NAME: model.layers.23.mlp.act_fn MODULE: SiLU()\n",
      "NAME: model.layers.23.input_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.layers.23.post_attention_layernorm MODULE: LlamaRMSNorm()\n",
      "NAME: model.norm MODULE: LlamaRMSNorm()\n",
      "NAME: lm_head MODULE: Linear(in_features=2048, out_features=32256, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(f'NAME: {name} MODULE: {module}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    # Check if the module is the one you want to add a hook to\n",
    "    # print(name)\n",
    "    if name == 'model.layers.22':\n",
    "        handle = module.register_forward_hook(print_dims)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.remove_forward_hook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hook(name):\n",
    "    def print_submodule_name_hook(module, input, output):\n",
    "        print(f\"{module.__class__.__name__} ({name})\")\n",
    "    return print_submodule_name_hook\n",
    "\n",
    "for name, submodule in model.named_modules():\n",
    "    submodule.register_forward_hook(create_hook(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(input_text, return_tensors=\"pt\")['input_ids'].to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding (lm_head)\n",
      "Embedding (model.embed_tokens)\n",
      "Embedding (model.embed_tokens)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.0.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.0.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.0.self_attn.q_proj)\n",
      "Linear (model.layers.0.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.0.self_attn.k_proj)\n",
      "Linear (model.layers.0.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.0.self_attn.v_proj)\n",
      "Linear (model.layers.0.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.0.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.0.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.0.self_attn.o_proj)\n",
      "Linear (model.layers.0.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.0.self_attn)\n",
      "LlamaSdpaAttention (model.layers.0.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.0.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.0.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.0.mlp.gate_proj)\n",
      "Linear (model.layers.0.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.0.mlp.act_fn)\n",
      "SiLU (model.layers.0.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.0.mlp.up_proj)\n",
      "Linear (model.layers.0.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.0.mlp.down_proj)\n",
      "Linear (model.layers.0.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.0.mlp)\n",
      "LlamaMLP (model.layers.0.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.0)\n",
      "LlamaDecoderLayer (model.layers.0)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.1.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.1.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.1.self_attn.q_proj)\n",
      "Linear (model.layers.1.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.1.self_attn.k_proj)\n",
      "Linear (model.layers.1.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.1.self_attn.v_proj)\n",
      "Linear (model.layers.1.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.1.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.1.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.1.self_attn.o_proj)\n",
      "Linear (model.layers.1.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.1.self_attn)\n",
      "LlamaSdpaAttention (model.layers.1.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.1.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.1.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.1.mlp.gate_proj)\n",
      "Linear (model.layers.1.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.1.mlp.act_fn)\n",
      "SiLU (model.layers.1.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.1.mlp.up_proj)\n",
      "Linear (model.layers.1.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.1.mlp.down_proj)\n",
      "Linear (model.layers.1.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.1.mlp)\n",
      "LlamaMLP (model.layers.1.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.1)\n",
      "LlamaDecoderLayer (model.layers.1)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.2.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.2.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.2.self_attn.q_proj)\n",
      "Linear (model.layers.2.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.2.self_attn.k_proj)\n",
      "Linear (model.layers.2.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.2.self_attn.v_proj)\n",
      "Linear (model.layers.2.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.2.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.2.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.2.self_attn.o_proj)\n",
      "Linear (model.layers.2.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.2.self_attn)\n",
      "LlamaSdpaAttention (model.layers.2.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.2.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.2.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.2.mlp.gate_proj)\n",
      "Linear (model.layers.2.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.2.mlp.act_fn)\n",
      "SiLU (model.layers.2.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.2.mlp.up_proj)\n",
      "Linear (model.layers.2.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.2.mlp.down_proj)\n",
      "Linear (model.layers.2.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.2.mlp)\n",
      "LlamaMLP (model.layers.2.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.2)\n",
      "LlamaDecoderLayer (model.layers.2)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.3.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.3.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.3.self_attn.q_proj)\n",
      "Linear (model.layers.3.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.3.self_attn.k_proj)\n",
      "Linear (model.layers.3.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.3.self_attn.v_proj)\n",
      "Linear (model.layers.3.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.3.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.3.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.3.self_attn.o_proj)\n",
      "Linear (model.layers.3.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.3.self_attn)\n",
      "LlamaSdpaAttention (model.layers.3.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.3.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.3.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.3.mlp.gate_proj)\n",
      "Linear (model.layers.3.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.3.mlp.act_fn)\n",
      "SiLU (model.layers.3.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.3.mlp.up_proj)\n",
      "Linear (model.layers.3.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.3.mlp.down_proj)\n",
      "Linear (model.layers.3.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.3.mlp)\n",
      "LlamaMLP (model.layers.3.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.3)\n",
      "LlamaDecoderLayer (model.layers.3)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.4.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.4.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.4.self_attn.q_proj)\n",
      "Linear (model.layers.4.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.4.self_attn.k_proj)\n",
      "Linear (model.layers.4.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.4.self_attn.v_proj)\n",
      "Linear (model.layers.4.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.4.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.4.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.4.self_attn.o_proj)\n",
      "Linear (model.layers.4.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.4.self_attn)\n",
      "LlamaSdpaAttention (model.layers.4.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.4.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.4.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.4.mlp.gate_proj)\n",
      "Linear (model.layers.4.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.4.mlp.act_fn)\n",
      "SiLU (model.layers.4.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.4.mlp.up_proj)\n",
      "Linear (model.layers.4.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.4.mlp.down_proj)\n",
      "Linear (model.layers.4.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.4.mlp)\n",
      "LlamaMLP (model.layers.4.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.4)\n",
      "LlamaDecoderLayer (model.layers.4)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.5.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.5.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.5.self_attn.q_proj)\n",
      "Linear (model.layers.5.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.5.self_attn.k_proj)\n",
      "Linear (model.layers.5.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.5.self_attn.v_proj)\n",
      "Linear (model.layers.5.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.5.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.5.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.5.self_attn.o_proj)\n",
      "Linear (model.layers.5.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.5.self_attn)\n",
      "LlamaSdpaAttention (model.layers.5.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.5.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.5.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.5.mlp.gate_proj)\n",
      "Linear (model.layers.5.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.5.mlp.act_fn)\n",
      "SiLU (model.layers.5.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.5.mlp.up_proj)\n",
      "Linear (model.layers.5.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.5.mlp.down_proj)\n",
      "Linear (model.layers.5.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.5.mlp)\n",
      "LlamaMLP (model.layers.5.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.5)\n",
      "LlamaDecoderLayer (model.layers.5)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.6.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.6.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.6.self_attn.q_proj)\n",
      "Linear (model.layers.6.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.6.self_attn.k_proj)\n",
      "Linear (model.layers.6.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.6.self_attn.v_proj)\n",
      "Linear (model.layers.6.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.6.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.6.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.6.self_attn.o_proj)\n",
      "Linear (model.layers.6.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.6.self_attn)\n",
      "LlamaSdpaAttention (model.layers.6.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.6.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.6.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.6.mlp.gate_proj)\n",
      "Linear (model.layers.6.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.6.mlp.act_fn)\n",
      "SiLU (model.layers.6.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.6.mlp.up_proj)\n",
      "Linear (model.layers.6.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.6.mlp.down_proj)\n",
      "Linear (model.layers.6.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.6.mlp)\n",
      "LlamaMLP (model.layers.6.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.6)\n",
      "LlamaDecoderLayer (model.layers.6)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.7.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.7.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.7.self_attn.q_proj)\n",
      "Linear (model.layers.7.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.7.self_attn.k_proj)\n",
      "Linear (model.layers.7.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.7.self_attn.v_proj)\n",
      "Linear (model.layers.7.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.7.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.7.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.7.self_attn.o_proj)\n",
      "Linear (model.layers.7.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.7.self_attn)\n",
      "LlamaSdpaAttention (model.layers.7.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.7.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.7.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.7.mlp.gate_proj)\n",
      "Linear (model.layers.7.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.7.mlp.act_fn)\n",
      "SiLU (model.layers.7.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.7.mlp.up_proj)\n",
      "Linear (model.layers.7.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.7.mlp.down_proj)\n",
      "Linear (model.layers.7.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.7.mlp)\n",
      "LlamaMLP (model.layers.7.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.7)\n",
      "LlamaDecoderLayer (model.layers.7)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.8.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.8.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.8.self_attn.q_proj)\n",
      "Linear (model.layers.8.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.8.self_attn.k_proj)\n",
      "Linear (model.layers.8.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.8.self_attn.v_proj)\n",
      "Linear (model.layers.8.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.8.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.8.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.8.self_attn.o_proj)\n",
      "Linear (model.layers.8.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.8.self_attn)\n",
      "LlamaSdpaAttention (model.layers.8.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.8.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.8.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.8.mlp.gate_proj)\n",
      "Linear (model.layers.8.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.8.mlp.act_fn)\n",
      "SiLU (model.layers.8.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.8.mlp.up_proj)\n",
      "Linear (model.layers.8.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.8.mlp.down_proj)\n",
      "Linear (model.layers.8.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.8.mlp)\n",
      "LlamaMLP (model.layers.8.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.8)\n",
      "LlamaDecoderLayer (model.layers.8)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.9.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.9.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.9.self_attn.q_proj)\n",
      "Linear (model.layers.9.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.9.self_attn.k_proj)\n",
      "Linear (model.layers.9.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.9.self_attn.v_proj)\n",
      "Linear (model.layers.9.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.9.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.9.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.9.self_attn.o_proj)\n",
      "Linear (model.layers.9.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.9.self_attn)\n",
      "LlamaSdpaAttention (model.layers.9.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.9.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.9.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.9.mlp.gate_proj)\n",
      "Linear (model.layers.9.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.9.mlp.act_fn)\n",
      "SiLU (model.layers.9.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.9.mlp.up_proj)\n",
      "Linear (model.layers.9.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.9.mlp.down_proj)\n",
      "Linear (model.layers.9.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.9.mlp)\n",
      "LlamaMLP (model.layers.9.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.9)\n",
      "LlamaDecoderLayer (model.layers.9)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.10.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.10.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.10.self_attn.q_proj)\n",
      "Linear (model.layers.10.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.10.self_attn.k_proj)\n",
      "Linear (model.layers.10.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.10.self_attn.v_proj)\n",
      "Linear (model.layers.10.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.10.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.10.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.10.self_attn.o_proj)\n",
      "Linear (model.layers.10.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.10.self_attn)\n",
      "LlamaSdpaAttention (model.layers.10.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.10.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.10.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.10.mlp.gate_proj)\n",
      "Linear (model.layers.10.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.10.mlp.act_fn)\n",
      "SiLU (model.layers.10.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.10.mlp.up_proj)\n",
      "Linear (model.layers.10.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.10.mlp.down_proj)\n",
      "Linear (model.layers.10.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.10.mlp)\n",
      "LlamaMLP (model.layers.10.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.10)\n",
      "LlamaDecoderLayer (model.layers.10)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.11.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.11.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.11.self_attn.q_proj)\n",
      "Linear (model.layers.11.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.11.self_attn.k_proj)\n",
      "Linear (model.layers.11.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.11.self_attn.v_proj)\n",
      "Linear (model.layers.11.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.11.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.11.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.11.self_attn.o_proj)\n",
      "Linear (model.layers.11.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.11.self_attn)\n",
      "LlamaSdpaAttention (model.layers.11.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.11.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.11.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.11.mlp.gate_proj)\n",
      "Linear (model.layers.11.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.11.mlp.act_fn)\n",
      "SiLU (model.layers.11.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.11.mlp.up_proj)\n",
      "Linear (model.layers.11.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.11.mlp.down_proj)\n",
      "Linear (model.layers.11.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.11.mlp)\n",
      "LlamaMLP (model.layers.11.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.11)\n",
      "LlamaDecoderLayer (model.layers.11)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.12.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.12.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.12.self_attn.q_proj)\n",
      "Linear (model.layers.12.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.12.self_attn.k_proj)\n",
      "Linear (model.layers.12.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.12.self_attn.v_proj)\n",
      "Linear (model.layers.12.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.12.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.12.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.12.self_attn.o_proj)\n",
      "Linear (model.layers.12.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.12.self_attn)\n",
      "LlamaSdpaAttention (model.layers.12.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.12.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.12.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.12.mlp.gate_proj)\n",
      "Linear (model.layers.12.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.12.mlp.act_fn)\n",
      "SiLU (model.layers.12.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.12.mlp.up_proj)\n",
      "Linear (model.layers.12.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.12.mlp.down_proj)\n",
      "Linear (model.layers.12.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.12.mlp)\n",
      "LlamaMLP (model.layers.12.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.12)\n",
      "LlamaDecoderLayer (model.layers.12)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.13.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.13.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.13.self_attn.q_proj)\n",
      "Linear (model.layers.13.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.13.self_attn.k_proj)\n",
      "Linear (model.layers.13.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.13.self_attn.v_proj)\n",
      "Linear (model.layers.13.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.13.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.13.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.13.self_attn.o_proj)\n",
      "Linear (model.layers.13.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.13.self_attn)\n",
      "LlamaSdpaAttention (model.layers.13.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.13.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.13.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.13.mlp.gate_proj)\n",
      "Linear (model.layers.13.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.13.mlp.act_fn)\n",
      "SiLU (model.layers.13.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.13.mlp.up_proj)\n",
      "Linear (model.layers.13.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.13.mlp.down_proj)\n",
      "Linear (model.layers.13.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.13.mlp)\n",
      "LlamaMLP (model.layers.13.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.13)\n",
      "LlamaDecoderLayer (model.layers.13)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.14.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.14.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.14.self_attn.q_proj)\n",
      "Linear (model.layers.14.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.14.self_attn.k_proj)\n",
      "Linear (model.layers.14.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.14.self_attn.v_proj)\n",
      "Linear (model.layers.14.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.14.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.14.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.14.self_attn.o_proj)\n",
      "Linear (model.layers.14.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.14.self_attn)\n",
      "LlamaSdpaAttention (model.layers.14.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.14.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.14.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.14.mlp.gate_proj)\n",
      "Linear (model.layers.14.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.14.mlp.act_fn)\n",
      "SiLU (model.layers.14.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.14.mlp.up_proj)\n",
      "Linear (model.layers.14.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.14.mlp.down_proj)\n",
      "Linear (model.layers.14.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.14.mlp)\n",
      "LlamaMLP (model.layers.14.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.14)\n",
      "LlamaDecoderLayer (model.layers.14)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.15.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.15.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.15.self_attn.q_proj)\n",
      "Linear (model.layers.15.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.15.self_attn.k_proj)\n",
      "Linear (model.layers.15.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.15.self_attn.v_proj)\n",
      "Linear (model.layers.15.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.15.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.15.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.15.self_attn.o_proj)\n",
      "Linear (model.layers.15.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.15.self_attn)\n",
      "LlamaSdpaAttention (model.layers.15.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.15.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.15.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.15.mlp.gate_proj)\n",
      "Linear (model.layers.15.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.15.mlp.act_fn)\n",
      "SiLU (model.layers.15.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.15.mlp.up_proj)\n",
      "Linear (model.layers.15.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.15.mlp.down_proj)\n",
      "Linear (model.layers.15.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.15.mlp)\n",
      "LlamaMLP (model.layers.15.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.15)\n",
      "LlamaDecoderLayer (model.layers.15)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.16.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.16.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.16.self_attn.q_proj)\n",
      "Linear (model.layers.16.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.16.self_attn.k_proj)\n",
      "Linear (model.layers.16.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.16.self_attn.v_proj)\n",
      "Linear (model.layers.16.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.16.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.16.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.16.self_attn.o_proj)\n",
      "Linear (model.layers.16.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.16.self_attn)\n",
      "LlamaSdpaAttention (model.layers.16.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.16.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.16.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.16.mlp.gate_proj)\n",
      "Linear (model.layers.16.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.16.mlp.act_fn)\n",
      "SiLU (model.layers.16.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.16.mlp.up_proj)\n",
      "Linear (model.layers.16.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.16.mlp.down_proj)\n",
      "Linear (model.layers.16.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.16.mlp)\n",
      "LlamaMLP (model.layers.16.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.16)\n",
      "LlamaDecoderLayer (model.layers.16)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.17.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.17.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.17.self_attn.q_proj)\n",
      "Linear (model.layers.17.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.17.self_attn.k_proj)\n",
      "Linear (model.layers.17.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.17.self_attn.v_proj)\n",
      "Linear (model.layers.17.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.17.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.17.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.17.self_attn.o_proj)\n",
      "Linear (model.layers.17.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.17.self_attn)\n",
      "LlamaSdpaAttention (model.layers.17.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.17.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.17.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.17.mlp.gate_proj)\n",
      "Linear (model.layers.17.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.17.mlp.act_fn)\n",
      "SiLU (model.layers.17.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.17.mlp.up_proj)\n",
      "Linear (model.layers.17.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.17.mlp.down_proj)\n",
      "Linear (model.layers.17.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.17.mlp)\n",
      "LlamaMLP (model.layers.17.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.17)\n",
      "LlamaDecoderLayer (model.layers.17)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.18.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.18.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.18.self_attn.q_proj)\n",
      "Linear (model.layers.18.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.18.self_attn.k_proj)\n",
      "Linear (model.layers.18.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.18.self_attn.v_proj)\n",
      "Linear (model.layers.18.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.18.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.18.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.18.self_attn.o_proj)\n",
      "Linear (model.layers.18.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.18.self_attn)\n",
      "LlamaSdpaAttention (model.layers.18.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.18.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.18.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.18.mlp.gate_proj)\n",
      "Linear (model.layers.18.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.18.mlp.act_fn)\n",
      "SiLU (model.layers.18.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.18.mlp.up_proj)\n",
      "Linear (model.layers.18.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.18.mlp.down_proj)\n",
      "Linear (model.layers.18.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.18.mlp)\n",
      "LlamaMLP (model.layers.18.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.18)\n",
      "LlamaDecoderLayer (model.layers.18)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.19.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.19.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.19.self_attn.q_proj)\n",
      "Linear (model.layers.19.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.19.self_attn.k_proj)\n",
      "Linear (model.layers.19.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.19.self_attn.v_proj)\n",
      "Linear (model.layers.19.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.19.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.19.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.19.self_attn.o_proj)\n",
      "Linear (model.layers.19.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.19.self_attn)\n",
      "LlamaSdpaAttention (model.layers.19.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.19.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.19.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.19.mlp.gate_proj)\n",
      "Linear (model.layers.19.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.19.mlp.act_fn)\n",
      "SiLU (model.layers.19.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.19.mlp.up_proj)\n",
      "Linear (model.layers.19.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.19.mlp.down_proj)\n",
      "Linear (model.layers.19.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.19.mlp)\n",
      "LlamaMLP (model.layers.19.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.19)\n",
      "LlamaDecoderLayer (model.layers.19)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.20.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.20.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.20.self_attn.q_proj)\n",
      "Linear (model.layers.20.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.20.self_attn.k_proj)\n",
      "Linear (model.layers.20.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.20.self_attn.v_proj)\n",
      "Linear (model.layers.20.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.20.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.20.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.20.self_attn.o_proj)\n",
      "Linear (model.layers.20.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.20.self_attn)\n",
      "LlamaSdpaAttention (model.layers.20.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.20.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.20.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.20.mlp.gate_proj)\n",
      "Linear (model.layers.20.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.20.mlp.act_fn)\n",
      "SiLU (model.layers.20.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.20.mlp.up_proj)\n",
      "Linear (model.layers.20.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.20.mlp.down_proj)\n",
      "Linear (model.layers.20.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.20.mlp)\n",
      "LlamaMLP (model.layers.20.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.20)\n",
      "LlamaDecoderLayer (model.layers.20)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.21.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.21.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.21.self_attn.q_proj)\n",
      "Linear (model.layers.21.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.21.self_attn.k_proj)\n",
      "Linear (model.layers.21.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.21.self_attn.v_proj)\n",
      "Linear (model.layers.21.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.21.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.21.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.21.self_attn.o_proj)\n",
      "Linear (model.layers.21.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.21.self_attn)\n",
      "LlamaSdpaAttention (model.layers.21.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.21.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.21.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.21.mlp.gate_proj)\n",
      "Linear (model.layers.21.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.21.mlp.act_fn)\n",
      "SiLU (model.layers.21.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.21.mlp.up_proj)\n",
      "Linear (model.layers.21.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.21.mlp.down_proj)\n",
      "Linear (model.layers.21.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.21.mlp)\n",
      "LlamaMLP (model.layers.21.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.21)\n",
      "LlamaDecoderLayer (model.layers.21)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.22.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.22.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.22.self_attn.q_proj)\n",
      "Linear (model.layers.22.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.22.self_attn.k_proj)\n",
      "Linear (model.layers.22.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.22.self_attn.v_proj)\n",
      "Linear (model.layers.22.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.22.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.22.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.22.self_attn.o_proj)\n",
      "Linear (model.layers.22.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.22.self_attn)\n",
      "LlamaSdpaAttention (model.layers.22.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.22.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.22.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.22.mlp.gate_proj)\n",
      "Linear (model.layers.22.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.22.mlp.act_fn)\n",
      "SiLU (model.layers.22.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.22.mlp.up_proj)\n",
      "Linear (model.layers.22.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.22.mlp.down_proj)\n",
      "Linear (model.layers.22.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.22.mlp)\n",
      "LlamaMLP (model.layers.22.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.22)\n",
      "LlamaDecoderLayer (model.layers.22)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.23.input_layernorm)\n",
      "LlamaRMSNorm (model.layers.23.input_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.23.self_attn.q_proj)\n",
      "Linear (model.layers.23.self_attn.q_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.23.self_attn.k_proj)\n",
      "Linear (model.layers.23.self_attn.k_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.23.self_attn.v_proj)\n",
      "Linear (model.layers.23.self_attn.v_proj)\n",
      "LlamaLinearScalingRotaryEmbedding (lm_head)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.23.self_attn.rotary_emb)\n",
      "LlamaLinearScalingRotaryEmbedding (model.layers.23.self_attn.rotary_emb)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.23.self_attn.o_proj)\n",
      "Linear (model.layers.23.self_attn.o_proj)\n",
      "LlamaSdpaAttention (lm_head)\n",
      "LlamaSdpaAttention (model.layers.23.self_attn)\n",
      "LlamaSdpaAttention (model.layers.23.self_attn)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.layers.23.post_attention_layernorm)\n",
      "LlamaRMSNorm (model.layers.23.post_attention_layernorm)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.23.mlp.gate_proj)\n",
      "Linear (model.layers.23.mlp.gate_proj)\n",
      "SiLU (lm_head)\n",
      "SiLU (model.layers.23.mlp.act_fn)\n",
      "SiLU (model.layers.23.mlp.act_fn)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.23.mlp.up_proj)\n",
      "Linear (model.layers.23.mlp.up_proj)\n",
      "Linear (lm_head)\n",
      "Linear (model.layers.23.mlp.down_proj)\n",
      "Linear (model.layers.23.mlp.down_proj)\n",
      "LlamaMLP (lm_head)\n",
      "LlamaMLP (model.layers.23.mlp)\n",
      "LlamaMLP (model.layers.23.mlp)\n",
      "LlamaDecoderLayer (lm_head)\n",
      "LlamaDecoderLayer (model.layers.23)\n",
      "LlamaDecoderLayer (model.layers.23)\n",
      "LlamaRMSNorm (lm_head)\n",
      "LlamaRMSNorm (model.norm)\n",
      "LlamaRMSNorm (model.norm)\n",
      "LlamaModel (lm_head)\n",
      "LlamaModel (model)\n",
      "LlamaModel (model)\n",
      "Linear (lm_head)\n",
      "Linear (lm_head)\n",
      "Linear (lm_head)\n",
      "LlamaForCausalLM (lm_head)\n",
      "LlamaForCausalLM ()\n",
      "LlamaForCausalLM ()\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('backpack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e552d307c8781d98a1345404e47beeec67d0c4ac0bb536f85b4d3cc6c64bd723"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
